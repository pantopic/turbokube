Surprises:
  etcd:
    - Network bandwidth is higher than expected
    - CPU usage in relation to other metrics is lower than expected
    - Memory usage is way lower than expected (under 0.5 GB out of 16)
  api-server:
    - Memory usage is about half of expected (4 GB out of 16)
  scheduler: 
    resource_requests:
      The turbokube scheduler is really bad at assigning pods to underutilized nodes. The entire process stalled many
      times after autoscale because the scheduler assigned new pods to older nodes even when there were new nodes
      with relatively few pods assigned. Maybe the lack of resource requests is breaking the scheduler algorithm and
      we need to request some minimum amount of memory in order for balance to occur?
    concurrency:
      A `-c` param was added to turboctl from the start so that namespaces could be provisioned in parallel. Once a
      clean run was produced surpassing 32k pods, turboctl was run with `-c 2` which should, in theory, double the
      rate at which pods and nodes are created. This did not happen. Instead, latency doubled and throughput remained
      almost unchanged. Turns out the rate at which we can add pods to the cluster is determined by the scheduler.

Not surprises:
  leases:
    - Lease renewal throughput grows linearly (as suspected)

2025-nov-16:
  - What are we really load testing here? Stretching past 150,000 pods when we're provisioning at 12 pods per second
    feels like a waste of time and money. Do we want to pay $40 for a 4 hour load test just to find the next bottleneck
    so we can run it again? What's the ROI? It seems like the default scheduler is just going to 
    [desample](https://kubernetes.io/docs/concepts/scheduling-eviction/scheduler-perf-tuning/#percentage-of-nodes-to-score) 
    as node count increases to maintain consistent performance anyways.  
    Wouldn't it be faster, cheaper and more useful to accelerate the test by improving the throughput of the scheduler?
    Isn't that the original complaint? Throughput rather than maximum capacity?

2025-nov-17:
  - Adding a bunch of schedulers didn't increase throughput at all. All it did is occupy more memory.
  - Pegged at 16 pods per second no matter what. This is exciting.
  - API server memory util is nearly 50% of 16GB beyond 64k pods.
  - Etcd still appears not to be a bottleneck.
  - Worker pool was not correctly balanced. More nodes with less ram and lower scaling thresholds will be better.
  - The very predictable throughput makes this start to look like some kind of lock contention issue.
  - It might be time to attack the api server (watch cache).
  - Tuning params?: # https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/
    - --profiling
    - --max-requests-inflight          (2025-11-18T15-01-37Z - no effect)
    - --max-mutating-requests-inflight (2025-11-18T15-01-37Z - no effect)
    - --watch-cache                    (2025-11-18T15-50-40Z - no effect but huge network spike)
    - --watch-cache-sizes
    - --storage-backend?
  - Let's get krv running and experiment with read replication, then mess with the wach cache.
  - Cloud bill so far is $80

2025-nov-18:
  - Network throughput with the watch cache disabled is insane. 300Mbps network outbound from < 3Mbps disk.
  - Etcd 1500 nodes (48k pods) = 566M snap, 367M wal. Status reports 602 MB.
  - Api server is pegged

2025-nov-19:
  - It's alive!
  - Etcd 1500 nodes (48k pods) = 772M data, 129M raft. Status reports 772 MB.
  - Stalled worker paused test at 3.2k nodes, 100k pods. Killed worker to resume. PodCidr conflict.
  - Reduce pod cider from /20 to /21 and increase worker limit from 16 to 32
  - Krv memory usage is about the same as the database size.
  - Prometheus starts to freak out around 114k pods w/ 8 pegged cores @ 38GB memory usage
  - Increase raft compaction limit from 10k to 100k to reduce disk io
  - etcd-2 ran out of ram. Unable to recover. Absolute panic. 3700 nodes, 116k pods
  - |
    (altered) > iostat -d -x 5 -z
    Device            r/s     rkB/s   rrqm/s  %rrqm r_await rareq-sz     w/s     wkB/s   wrqm/s  %wrqm w_await wareq-sz     d/s     dkB/s   drqm/s  %drqm d_await dareq-sz     f/s f_await  aqu-sz  %util
    vda              0.00      0.00     0.00   0.00    0.00     0.00 6792.60  31069.60   766.60  10.14    0.48     4.57    0.00      0.00     0.00   0.00    0.00     0.00 1155.20    0.04    3.28  60.16
  - |
    (unaltered) > iostat -d -x 5 -z
    Device            r/s     rkB/s   rrqm/s  %rrqm r_await rareq-sz     w/s     wkB/s   wrqm/s  %wrqm w_await wareq-sz     d/s     dkB/s   drqm/s  %drqm d_await dareq-sz     f/s f_await  aqu-sz  %util
    vda              0.00      0.00     0.00   0.00    0.00     0.00 1354.60   7472.80   508.00  27.27    0.09     5.52    0.00      0.00     0.00   0.00    0.00     0.00  902.80    0.04    0.16  57.60

2025-nov-20:
  - Fixed a bug preventing the shard controller from scheduling read replicas. See `Placement triggers` for long term fix.
  - Fixed a bug in kvr snapshot (lmdb env folder -> file)
  - Kube-controller-manager is hammering apiserver-0 with asymmetric load. Need to isolate it.
  - Read replica test went well. Local consistent reads reduced bandwidth.
  - Disabling watch cache will be fun.
  - Extracting lease expiration will be funner.
  - Provisioning rate is locked but steady at ~1024 pods per minute. Scheduler queue is locked.
  - Possible scheduler improvements:
    - Batch pod scheduling
    - Add support for multiple scheduling queues during PreEnqueue

2025-nov-28:
  - 4,276 nodes, 136,832 pods 
  - API Server ran out of CPU
  - Scheduler using more cpu than apiserver. 
  - Turbo also running low on CPU
  - 50+ pods per second not bad
  - Flannel's pod cidr range is limited to /16 (64k)
  - Try out Calico after we surpass 32k nodes
  - Next run:
    - Vertically scale API Servers and turbo control plane
    - Fix offloaded schedulers

2025-dec-01:
  - 150 pods/sec w/ 4 schedulers
  - Over 6k vnodes and 200k pods
  - etcd disk is the bottleneck, 1k ops/s steady @ 30% cpu & 95% disk util
  - scheduler instances are pegged, scale from 2 to 4 instances, 4 to 8 pods
  - apiserver-0 is getting low on ram (controller manager) upgrade to m-8vcpu-64gb-intel
  - krv storage 4.7G @ 8,192 x 32
