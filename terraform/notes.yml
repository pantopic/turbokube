Surprises:
  etcd:
    - Network bandwidth is higher than expected
    - CPU usage in relation to other metrics is lower than expected
    - Memory usage is way lower than expected (under 0.5 GB out of 16)
  api-server:
    - Memory usage is about half of expected (4 GB out of 16)
  scheduler: 
    resource_requests:
      The turbokube scheduler is really bad at assigning pods to underutilized nodes. The entire process stalled many
      times after autoscale because the scheduler assigned new pods to older nodes even when there were new nodes
      with relatively few pods assigned. Maybe the lack of resource requests is breaking the scheduler algorithm and
      we need to request some minimum amount of memory in order for balance to occur?
    concurrency:
      A `-c` param was added to turboctl from the start so that namespaces could be provisioned in parallel. Once a
      clean run was produced surpassing 32k pods, turboctl was run with `-c 2` which should, in theory, double the
      rate at which pods and nodes are created. This did not happen. Instead, latency doubled and throughput remained
      almost unchanged. Turns out the rate at which we can add pods to the cluster is determined by the scheduler.

Not surprises:
  leases:
    - Lease renewal throughput grows linearly (as suspected)

2025-nov-16:
  - What are we really load testing here? Stretching past 150,000 pods when we're provisioning at 12 pods per second
    feels like a waste of time and money. Do we want to pay $40 for a 4 hour load test just to find the next bottleneck
    so we can run it again? What's the ROI? It seems like the default scheduler is just going to 
    [desample](https://kubernetes.io/docs/concepts/scheduling-eviction/scheduler-perf-tuning/#percentage-of-nodes-to-score) 
    as node count increases to maintain consistent performance anyways.  
    Wouldn't it be faster, cheaper and more useful to accelerate the test by improving the throughput of the scheduler?
    Isn't that the original complaint? Throughput rather than maximum capacity?

2025-nov-17:
  - Adding a bunch of schedulers didn't increase throughput at all. All it did is occupy more memory.
  - Pegged at 16 pods per second no matter what. This is exciting.
  - API server memory util is nearly 50% of 16GB beyond 64k pods.
  - Etcd still appears not to be a bottleneck.
  - Worker pool was not correctly balanced. More nodes with less ram and lower scaling thresholds will be better.
  - The very predictable throughput makes this start to look like some kind of lock contention issue.
  - It might be time to attack the api server (watch cache).
  - Tuning params?: # https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/
    - --profiling
    - --max-requests-inflight          (2025-11-18T15-01-37Z - no effect)
    - --max-mutating-requests-inflight (2025-11-18T15-01-37Z - no effect)
    - --watch-cache                    (2025-11-18T15-50-40Z - no effect but huge network spike)
    - --watch-cache-sizes
    - --storage-backend?
  - Let's get krv running and experiment with read replication, then mess with the wach cache.
  - Cloud bill so far is $80

2025-nov-18:
  - Network throughput with the watch cache disabled is insane. 300Mbps network outbound from < 3Mbps disk.
  - Etcd 1500 nodes (48k pods) = 566M snap, 367M wal. Status reports 602 MB.
  - Api server is pegged
